====================================================================================================01 IAM
IAM = identity access management
iam consists of users groups roles policy documents
[effect:allow, actions:*, resource:*] <=  //allows acces to everything in aws
this is a policy document 
policy docs are in in json 
json is a collection of keyvalue pairs.
IAM is universial.
universal means- not apply to regions at this time
root acount - is the account register when first setup aws account
the root account has complete admin access
you should create new users instead of using the root account
new users have no permissions when first created
new users are ssigned access key id & secret access keys when first created
you can not use them to login to aws managment console. 
that uses different credentials.
you can use it to access API and Console from desktop though.
you can only view access key id & secret access key once.
if you lose them you will have to regenerate them.
always setup multifactor authentication on your root account
you can create and customise your own password rotations policies.
=============================================================================================================================================02 EC2
there are pricing models or EC2
on demand - fixed rate by hour or second with no commitment
reserved - provides you with a capcity reservations,
 and offer a significant discount on the hourly charge for an instance.
  1 or 3year terms
spot  - enables you to bid whateer price you want ofr instance capacity
 proiding even greate savings if your application has flexible start end times
dedicated hosts- physical ec2 server didcated for your use.
dedicated hosts help you reduce cost by allowing you to use your existing server-bound software licenses.
if a spot instance is terminated by Amazon Ec2 you will not be caged for a partial hour of usage.
but if you terminate ec2 instance yourself you will be charged for the complete hour.
 mnemonic for ec2 instance types are FIGHT DR MC PX
 -
there are ebds volumes.
two kinds of SSD:
general perpose SSD - balances price nd perforance for a widevariety of workloads
provisioned IOPS SSD highest-performance SSD volume for mission cirtical low latency or high throughput workloads
three kind of magnetic:
throughput optimized HDD - low cost HDD volume designed ofr frequently accessed througput-intensive workloads
cold hdd - lowes cost hdd volume designed for less frequently accessed workloads
magnetic - previous generation . can be a boot volume.
there are 3 types of load balancers
application network and classic
504 error mean the gateway timed out.
which means the applications did not respod withn the idle timeout period.
trouble shoot the stack ? is it web server or database server?
if you need IPv4 address of your end user look ofr the x-Forwarded-For header
route53 is amazons dns service
route53 allow you to map your domain names to Ec2 isntances loadbalancers or s3buckets
least privilege - always give your users the minimum amount of access required
create groups - assign users to groups
users automatically inherit the permissions of the group
give each develop their own key dont share them among developers
roles allow you to not use access key id and secret access keys
roles are preferred form a security perspective
when you change a policy on a role it has immediate affect
you can deattach & attach roles to runing ec2 instances without stoping it.
you can encrypt the root device volume with OS level encryption
the root device volume is where the OS is installed on aws
to encryt a volume during create  or else you will have 3 step process.
the 3 step process is 1. snapshot the vollume. 2. make AMI of the snapshot. 3 deploy the encrypted root device volume
rds is for online transaction processing OLTP
examples are sql mysql postgresql oracle aurora mariadb
dynamodb is nosql 
redshit is online analytics processing(OLAP)
elasticache is an in memory caching engine
elasticache consists of memcache-d & redis
multi-AZ is only for disacter recovery not for improving performance
for performance improvement you need read replicas
read replicas are for scaling
you must have automatic backups turned on in order to deploy read replicas
you can have up to 5 read replica copies of any datapase
you can have read replicas of read replicas but watch out for latancy
each read replica has its own dns endpoint
you can have read replicas that have multi az
read replicas can be promoted to be their own database
promoting read replicas will break replication
you can have read replica in a second region (for mysql and mardiadb)
the exam will contain scenarios where a database is under heay load and you have to decide to use elastiche or redshit to alleviate it.
elasticache is good coice if your database is particularly read-heavy and not prone to frequent changing
redshift is good if the reason for the heavyload is managment keep runnign olap transactions on it etc. 
use memcahed if  1. object caching is primary goal 
use memcahed if 2. you want to keep things as simple as possible. 
use memcahed if 3. you want to scale your cache horizontally(scale out not up)

use redis 1. if you have advanced datatypes such as lists hashes and sets.
use redis2. if you are doing data sorting and raking such as leader booards
use redis3. if you want data persistence
use redis4. if you want multiaz
user redis5. if pub/sub capabilities are needed

====================================================================================================03 S3

s3 is object based i.e. lets you upload files
files can be 0 bytes t 5TB
there is unlimited storage
files stored in buckets
S3 is universial namespace. names must be uniue globally

read after write consistecy for PUTS of new objects
evetual consistency for overwrite PUTS and DELETES(can take some time to propagate) i.e. it may take a while to update
there are s3 storage tiers.
s3 durable immediately available, frequently accessed
s3-IA durable immediately available infrequently accessed
S3 - one zone IA same as IA but data is stored in single availability zone
S3 reduced redundancy storage - data that is easily reproducible such as thumbnails
glacier- archived data where you can wit 3-5 hours before accessing
there are the core fundamentals of an s3 object
they are key(name) value(data) version iD metadata subresources
subresouces of s3 object are BUcket policies ACLs ,CORS, transferacceleration
subresources are used to manage bucket-specific configuration
s3 is for object(files) storage only (not OS or databases)
s3 returns HTTP 200 on successful uploads
by default newly created buckets are private
you setup access control for bucket using bucket policies or Access control lists.
bucket polices are applied at bucket level
access crontrol lists are applied at an object level
s3 buckets can be configured to create access logs which log all requests made to the s3 bucket .
these logs can be written to another bucket
s3 suppors encryption in transit and encryption at rest
encryption in-transit is SSL/TLS
encryption at rest is server side encryption & client side encryption
client side encryption is where you encrypt file before upload to s3
servider side encryption is SSE-S3 SSE-KMS SSE-C
SSE-S3 
   -is each object is encrypted with a unique key (using strong multifactor encryption) 
   -keys are managed within S3
   -AWS handles all encryption-decryption for you end2end 
   -also called advanced encryption standard 256 bit
SSE-KMS is like ss2-s3 : 
   -you get an envelope key that protect encryption key for added protection
   -you also get audit trail that tells when keys were used and by whom
SSE-C is customer provided key where you manage own key
   you have to manage key life cycle yourself when it expires
you can create a bucket policy to prevent unencrypted files from being uploaded
cors stands for cross origin resource sharing
CORS is used to enable cross origin access for your aws resources
by default resurces in one buket cannot access resources located in another
to enable cors: go to the bucket being accessed and enable access for the origin (bucket attempting access) 
always use the s3 website url not the regular bucket url 
edge location - is the location where content is cached. 
ede location is different from AWS region/AZ
origin - origin of all files that CDN will distribute
origin can be an S3 bucket ec2 instance elastic loadbalancer or route53
distribution- name given to the CDN which consists of a collection of edge locations 
their are two types fo distributions
web distribution - typically used for websites
RTMP - is a distribution used for media streaming
Edge locations are not just READ only, you can write too : put objects in them
objects are cahced for the life of the TTL(time to live)
you can clear cached objects but you will be charged.
 there are two main approaches to performance optimization for S3
 for get-intensive workloads use cloadfront for performance
 for mixed-workloads avvoid sequential key names for you s3 objects(outdated)
todo: read the s3 faq on aws.amazon.com/s3/faqs
====================================================================================================04 serverless
lambda scales out(not up) automatically
lambda functions are all independent so 1 event = 1fuction
kambda is serverless
todo : know what services are serverless !
lambda function can trigger other lambda functions
architectures can get extremely complicated, AWS X-ray allows you to debug what is happening.
lambda can do things globally for exmple you can use it to backup s3 buckets to other s3 buckets in other regions
todo : know the lambda triggers. know what services can triger lambda.
todo : remember what API gateway is at a highlevel
api gateway has cahing capabilities to increase performance.
api gateway is low cost and scales automatically
you can throttle api gateway to prevent attacks
you can log resutls to cloudwatch
if you are usung javaript/ajax that uses multiple domains iwth api gateway ensure that you have enabled cors on api gateway
cors is enforced by the client
you can have multiple versions of lambda functions
lates lambda ersion will use $latest
qualified lambda version will use $latest, unqualified will not have it at the end of the ARN.
versions are immutale (cannot be changed)
can split traffic using alieases to different versions
but canot split traffic with $latest so point an alias to $lates instead.
can only split traffic two ways, not three.
step functions are a great way to visualize your serverless application
step functions automatically triggers and tracks each step
step functions log the state of each step so easy to track where something goes wrong
X-ray sdk provies interceptors to add to your code to trace incoming http requests.
xray SDK provides handlers to instrument aws sdk clients that your application uses to call other aws services
xray sdk provides an http client to instrment calls to other internal and external http web services.
xray integres with Elasti Loab balancing, aws lambda, amaon api gateway, amazon EC2, and elasti beanstalk
xray supports the following languges java go node.js python ruby .net
====================================================================================================05 dynamo db
amazon dynamodb is a low-laency nosql database
dynamodb consists of tales items and attributes
dynamodb supports both document and key-value data models
 dynamodb supported document formats are json html xml
2 types of primary keys  partition key and combination of partition + sortkey(composit key)
-----------------------------
2 consistency models : strongly consistent / eventually consistant(default)
strongly consistent reads if used if always need most upto date data.
access is controlled using IAM policies
fine grained access control using IAM condition parameter: dynamodb:LeadingKeys to allow uers to access only te items where the partition key value matches their user ID
------------------------------------------
indexes enable fast queries on speific data columns
give you a different view of your data based on alternative parition / sort keys
important to understand the differences
local seondary index: must be created at when you create your table, same partition key as your table, different sortkey
gloal secondary index: can create any time - at table reation or after , different partition key, different sort key.
----------------------------------
a query operation finds items in a table using only the primary key attribute
you provide the primary key name and a distinct alue to search for
a scan operation examines eery item in the table and by default returns all data attributes
use the projectionexpression parameter to refine the results
---------------------------
query results are always sorted by the sort key (if there is one)
sorted in ascending order
set scanindexforward parameter to false to reverse the order-queries only
query operation is generally more efficient than a scan
------------------------------
reduce the impact of a query or scan by setting a smaller page size which uses fewer read operations.
isolate scan operations to specific tables and segregate them from your mission-critical traffic.
try parallel sccans rather than the default sequential scan.
avoid using scan operations if you can design tables in a way that you can use the query, get, or batchgetitem apis
------------------------
privisioned throughpu tis measured in capacity units
1x write capacity unit = 1x1kb write persecond
1xread capacity unit = 1x4kb strongly consistent read or 2x4kb eentually consistent reads per second.
---------------------------
calculate write capacity requirments (100x512 byte items per second)
first calculate how many capacity units for each write
size of each item/1kb (for write capacity units) 512bytes/1kb = 0.5
rounded-up to the nearest whole number each write will need 1xwrite capacity unit per write operation
multiplied by the uber of writes per second = 1x1-- = 100 write capacity units required
--------------------------------
calculate read capacity requirements(80x3kb items per second)
first calculate how many capacity units for each read
size of each item /4kb for read capacity units 3kb/4kb=0.75
rounded-up to the nearest whole umer each read will eed 1xread capacity unit operation
multiplied by the numbe rof reads per second = 1x80=80 read capacity units required for strongly consistent but if eventualy consistency is cceptale diide by 2= 40 read capacity units required
-----------------------
DAX provides in -memory caching for dynamodb tables
DAX  improves response times for eventually consistent reads only
you point your api calls to the dax cluster instead of your table
if the item you are querying is on the cache dax will return it; 
 otherwise it will perform an eventually consistent getitem operation to your dynamodb table
 not suitable for write-intensive applications or applications that require strongy consistent reads.
 --------------------------
 elasticache = in-memory cache sits between your application and database
 2 different caching straegies: lazy loading and write through 
 lazy loading only caches the data when it is requested
 elasticach node  failures not fatal just lots of cache misses
 cache miss penalty  :(so overhead to ) initial request, query dataase,or  writing to cache 
 avoid stale data by implementing a TTL
 -----------------
 write through strategy writes data into the cache whenever there is a change to the database
 data is never stale
 write penalty : each write involves a write to the cache
 elasticache node failure means that data is missing util added or updated in the database
 wasted resources if most of the data is never used
====================================================================================================06 other aws services
sqs is a distributed message queuing system
sys allows you to decouple the components of an application so that they are ndependent
sqs is pull based not pushed based
there are two kinds of queues
standard queues(defaul): best effor ordering , message delivered at least once
FIFO queues : ording strictly preesrved, message delivered once, no duplicates (good for bank apps where order of transactions is important)
-----
sqs has visibility timeout
default visibility timeout is 30seconds,
you will need to  increases it if taskes takes longer than 30 seconds to complete
the max timeout is 12hours
shortpolling - returned immediately even if no messages are in the queue(might be expensive especially if not using it)
log polling - polls the queue periodically and only returns a response when a message is in the queue or go the timeout is reached
-----
sns is a sclable and highly available notification servvice which allows you to send push notifications to the cloud
sns supports a variety of formats: sms text messages, email, Amazon simple queue service queues, and http endpoint
sns is a pub-sub model where users subscribe to topics.
sns is push based rather than pull based.
SNS example archetecture:  a company wanting to send notifications ot multiple cutomers could use sns to fan out multiple messages in sqs format using a dedicated sqs queue per customer
------
ses is for email only
ses can be used for outgoing and incoming email
ses is not subscription based, you only need to know the email address
sns caters for sms sqs http email
sns uses push notifications only
sns uses pub sub model.
sns can fan out message to large number of recipients: ex multiple clients each with own sqs queue
----------
the are three core kinesis services
kinesis streams: video steams : securly stream video from connected devies to aws for analytics and machine learing
kinesis streams: data streams : built custom applications proess data in real time
kinesis firehose : capture transform and load data streams into aws data stores for near real time analytics with Business intelligence tools
you can configure lambda to subscribe to a kinesis stream and execute a function on your behalf when a new reord is deteced, before sending the processed data on to its final destination
-----
elastic bean stalk: deploys and scales your we appications including the web appllication server platform where required.
elastic bean stalk supports widley used programming technologies: java php python ruby go docker .net node.js
elastic bean stalk: supports application serer platforms like tomcat passenger puma and IIS
elastic bean stalk provisions the underlying resources for you
elastic bean stalk can fully manage the ec2 instances for you or you can take full administrative control
elastic beanstalk includes updates monitoring metrics and health checks  
------
there are 4 elasticbeanstalk deployment approaches
all at once:interrupt service, update entire environment at once. 
to roll back an all-at-once perform a further all at once upgrade.(probably only good for test&dev)
roolling : reduce capacity during deployment. to roll back perform a further rolling update.
rolling with aditional batch : maintains full capacity. to roll back perform a further rolling update
immutable: prefered option for mission critical productions systems.,maintains full capacity, to roll back just delete the new instances and autoscaling group.
-------
you can customize your elastic beanstalk environment using adding configuratinos files.
the elastic beanstalk files are written in yal or json.
the elastic beanstlak config files have a config extensions.
the elastic beanstalk config files are saved to the .ebextensions folder.
the .ebextensions folder must be located in the top level directory of your application source code bundle.
---------
there are two ways to launch rds instance
1. launch with elastic bean stalk
when you terminate elastic breanstalk envoironment the database will also be terminated
with elastic brean stalk it is quick and eay to add databases and get started
RDS & elastic brean stalk is suitable for dev&test only

2. lauch outside elastic bean stalk
outside elastic beanstalk requires addictional config steps like security group and conneciton information.
outside elastic beanstalk: suitable for production environments, more flexibility
outside elastic beantstalk: allow conection form multiple environments so you can tear down the application statck without imacting the database.
-------
====================================================================================================07 developer theory

continuous integration - integrating or merging the code changes frequently at least once per day. think code commit or github.
coninuous delivery - automating the build test and deployment functions : think codebuild and codedeploy.
continuous deployment : fully automated release process. 
code is deployed into staging or productin as soon as it has successfully passed throught he release pipeline. think code pipeline.
-------
todo : read aws white paper on practicing continuous integration & continuous deployment on aws
----
code commit is a centralized code repository 
code commit is a place to store source code binaries libraries images HTML files ect based on git.
codecommit enables collaboration - manages update form multiple users.
codecommit has versoin control - tracks and manages code changes , maintains version history.
------
know the different between in-place & blue/green deployments.
inplace: capacity is reduced during the deployment
inplace: lambda is not supported
inplace:rolling back involves a re-deploy
inplace:great when deploying first time.
blue/green: o capacity reduction
blue/green:green instances can be created ahead of time
blue/green: easy to switch between old and new
blue/green:you pay 2 environments intil you termiante old servers.
--------
appspec file defines the parameters to be used by code deploy ex: os,files,hoos.
the appspec.hyml file should be saed to the root of your revision.
hooks - lifecycle event hoos have a very specific run order.(called run order)
asspec.htl file has os section, files ections, beforeinstall afterinstall applicationstart validatserver.
------
in-place deployment as 3 phases.
the 3 phases of inplace deployment are de-registering, installation, re-registering with a load balancer.
todo : for exam you will need to know logical flow and the order in which thingsk happen.
------
beforelockTaffic : tasks you want to run on instances before tehy are de-registerd from a load balancer.
blocktraffic: de-register instances from a load balancer
afterblocktraffic:  tasks you want to run on instances after they are deregistered from a load balancer.
application stop : gracefully stop the application
downloadbundle: codedeploy agent copies the application revision files to a temprary locatoin.
beforeinstall : pre-installation scripts,ex backing up the current ersion, decrypting files
install: copy applicatino revisions files to final location.
after install: post-install scripts ex: configuration, file permissions.
applicationstart: start any services that twere stopped during applicationstop
validateservice- run tests to validate the service.
beforeallowtraffic: tasks you want to run on instances before they are registered with the load balancer
allowtraffic: register instances with a load balancer
afterallowtraffic: tasks you want to ru on instances after they are registered with a load balacer.

---
codepipeline: continguous integratoin/continuous deliver service 
codepipeline: orchestrates your end=to-end software release process based on a working flow you define.
codepipeline: automated : automatically triggers your pipline as soon as a chane is deteced in your source code repository.
codepipeline: integrates with other aws services & third-party tools like github
---------------
containers - virtual operating environment with eerything the software nees to run.
contaiers - includes libraries system tools code and runtime.
containers - allows applications to be built using independent stateless components or microservices ruing in multiple containers.
---------
ECS - container orchestration platform
ECS will ru your containers on clusters of virtual machines
FARGATE - serverless and can run containers
FARGATE - you dont need to worry about the underlying Ec2 isntance.
EC2 - can also run containers and allows more controls.
EC2 - use if you want to control the isntallation configuration and management of your compute environment.
ECR - stands for elastic container registry
ECR - this is where you can store your contianer images, docker or windows containers.
---------
todo : know docker commands ot build tag and push yoru docker image to an ECR repository.
------------
codebuild uses buildspec.yml
buildspec.yml define sthe build commands and setting used by codebuild to run your build
you can override settings in the buildspec.yml by adding your own commands in the console when you lauch the build.
build logs appear in the codebuild console and you can also iew the full codebuild log in cloudwatch.
-------------
cloud formation - allows you to manage configure and provisions aws infrastructure a scode YAML/JSON.
sections for a clode formation template
 are parameters (for custom values)
conditions (provision recources based on environment) 
resources: mandatory , the aws reouces to creat.
mappings - custom mapping for region AMI
transforms : refernce code located in S3 ex lambda code  , resuable snippets of code and also to specify the use of SAM
------------

sam stands for serverless application model
sam - defines and provisions serverless applications using cloudformation.
know the commands SAM PACKAGE and SAM DELPLOY
-----
cloud formations nested stacks- allows you to reuse CFcode so you dont need to copy paste.
these nested stacks are good for frequently used configs like loadbalancers, web and application servers. you refer to it instead of copy pasting.
to use nested stacks - store the template in S3 and  the resource type is stack.

====================================================================================================08 advanced IAM 
federation allows uers to authenticate with a web identity provide like google facebook amazon.

the user authenticates first with the web id provider and receives an authentication token, which is exchanged for temporary aws credentials allowing them to assume an iam.
------
cognito is an identity broker which handles interation between your applications and the web id provider(you dont need to write your own code to do this)
cognito provides signup sign-in and guest user access
cognito sysnc user data for a seamless experience across  your devices
cognito is the aws-recommended approach for web id federation.
------
cognito uses user pools to manage user sign-up and sign-in directly or via web identity providers.
cognito uses push synchronization to send a silent push notification of user data updates to multiple device types associated with a  userID
------
there are 3 types of iam policies.
managed policy - aws managed default policies
customer managed policy - managed by you
inline policy - managed by you and embedded in a single user group or role.
in most cases aws recommends using managed policies over inline policies.

====================================================================================================09 examp tips
CLI pagination - if you see errors like timed out or errors related to too many results being returned  then adjust the page size.
with reduced page size: the cli still retrieves the full list but performs a larger number of api calls in the bakground and retrieves a smaller number of items with each call.
------
IAM policy simulator lets  you test iam permissions before you commit them to production.
with IAM policy simulator you can validate that the policy works as expected.
with iam policy simulator you can test polcies already attached to existin gusers whic is great for trouble shooting.
-------------
SQS delay queue: can use to postponse delivery of new messages.
messages is the SQS delay queue: remain invisible for the duration of the delay period 0-900s
large distributed applications might need SQS  to introduce delay in processing to prevent errors.
with large messages upto  2gig =SQS cannot do this with  aws cli, aws management console/sqs or sqs api
------
kinesis shards - the kinesis client library on your consumers creates a record processor for each shard that is being consumed by your instance.
kiknesis shards: if you increate the number of shards the KCL will add more record processors on your consumers.
kiesis shards: CPU utilization is what should drive the quantity of consumer instances you have NOT the numbe rof shards in your kinesis stream.
kinesis shards: use an autoscaling grup and base scaling decisions on cpu load on your consumers.
------------
lambda concurrent executions limit: 1000 concurrent executions per second.
if you are running a busy serverlist website you will hit the concurrent executions limit probably.
if you hit the lambda concurrent executions limit you will see invocatoins being reject: 429 http status code.
from the console: request a limit raise by aws support if getting 429 error.
apply for reserved concurrency:guarantees a set number of concurrent executions are always available to a critical function.
------------
lambda versions : if your app uses alias instead of $latest it will not automcatically use new code when you upload it.
-----
lambda: is it possible to enable lambda to access recouces which are indise a private vpc.(but not default and will need to provide VPC config information)
--------
x-ray: integrates with many aws services like dynamadb lambda api gateway etc
x-ray: can also intrument your own app to send data to xray
x-ray: apps can be runing on ec2 elastic beanstalk evironments, on-premises systems and ecs
x-ray: for ECS , the xray daemon has to be run on its own doccker image, running along side your application. you would not expect a docker container to be running more than one microservice.
------
to configur xray you neee xray sdk, xray daemon, then instrument your app using the sdk to send data to xray 
with xray annotations you can record keyvalue pairs.